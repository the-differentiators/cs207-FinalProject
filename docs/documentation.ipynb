{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `ADKit`\n",
    "# Documentation\n",
    "\n",
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Derivatives are ubiquitous in many fields such as engineering design optimization, fluid dynamics and machine learning.\n",
    "There are in general three ways to calculate the derivatives: automatic differentiation, numeric differentiation, and\n",
    "symbolic differentiation. Automatic Differentiation (AD) brings a family of techniques that can calculate the partial\n",
    "derivatives of any function at any point efficiently and accurately. Unlike numeric differentiation, AD does not have\n",
    "the problem of floating point precision errors, since it calculates the derivative of a simple function, and keeps track of\n",
    "these derivatives, and there is no need of step sizes. Compared to symbolic differentiation, AD is not as memory intense,\n",
    "and can be much faster in terms of the calculation. Therefore, AD is an important way to calculate derivatives in\n",
    "practice.\n",
    "\n",
    "There are two modes in Automatic Differentiation: forward mode and the reverse mode. In forward mode, the chain rule is applied to each basic operation, and both the variable's value and derivative are calculated along the way, leading to a complete derivative trace. In reverse mode, there is a forward pass, where the intermediate variables are computed and their values and partial derivatives with respect to the previous layer stored in the memory, and also a reverse pass (popularly known as backward propagation), where we propagate back the derivatives with the help of the chain rule.\n",
    "\n",
    "The software that we design calculates the derivatives given the userâ€™s input using the forward mode/reverse mode of automatic differentiation depending on the user's choice,\n",
    "and provides the user with an easy way to solve their optimization problem using derivatives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "At the core of Automatic Differentiation is the principle that functions implemented as computer code can be broken down into elementary functions, ranging from arithmetic operations (e.g. addition, subtraction etc.) and other functions (e.g. power, exponential, sin etc.). Hence, any differentiable function can be interpreted as a composition of different functions. \n",
    "\n",
    "For example, given a function, $f = sin^2(2x)$, it can be rewritten as:\n",
    "\n",
    "$$ f = \\phi_1(\\phi_2(\\phi_3(x))) $$ \n",
    "\n",
    "where $$ \\phi_1(z) = z^2,   \\phi_2(y) = sin(y) \\text{ and } \\phi_3(x) = 2x$$\n",
    "\n",
    "\n",
    "In the forward mode, the chain rule can then be applied successively to each elementary component function to obtain the derivative of the function. Using the same example above, let $c$ be a real number:\n",
    "$$ f'(c) =  \\phi_3'(\\phi_2(\\phi_1(c))) \\cdot \\phi_2'(\\phi_1(c)) \\cdot \\phi_1'(c)$$\n",
    "\n",
    "Based on the example above, the derivative, $f'(c)$, can be evaluated based on the following function-derivative pairs at each stage of computing the function:\n",
    "\n",
    "$$(\\phi_1(c), \\phi_1'(c))$$\n",
    "\n",
    "$$(\\phi_2(\\phi_1(c)), (\\phi_2'(\\phi_1(c)) \\cdot \\phi_1'(c)))$$\n",
    "\n",
    "$$(\\phi_3(\\phi_2(\\phi_1(c))), \\phi_3'(\\phi_2(\\phi_1(c)) \\cdot \\phi_2'(\\phi_1(c)) \\cdot \\phi_1'(c))$$\n",
    "\n",
    "Effectively, the forward mode computes the Jacobian-vector product, $Jp$. This decomposition can be represented via a computational graph structure of calculations, requiring initial values to be set for $x_1$, and $x'_1$:\n",
    "\n",
    "$$x_1 \\rightarrow^{\\phi_3(x)} x_2 \\rightarrow^{\\phi_2(x)} x_3 \\rightarrow^{\\phi_1(x)} y $$\n",
    "\n",
    "where $$ \\phi_1(x) = x^2,   \\phi_2(x) = sin(x) \\text{ and } \\phi_3(x) = 2x$$\n",
    "\n",
    "At each stage of the function, the derivative of the function with respect to its argument is calculated. The exact values of the function and its derivative are used for the following function-derivative pair of values. An example of the computational trace for the equation $f = sin^2(2x)$ would look like this, for $x = \\dfrac{\\pi}{6}$. \n",
    "\n",
    "| Trace    | Elementary Operation &nbsp;&nbsp;&nbsp;| Derivative &nbsp;&nbsp;&nbsp; | $\\left(f\\left(a\\right),  f^{\\prime}\\left(a\\right)\\right)$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|\n",
    "| :------: | :----------------------:               | :------------------------------: | :------------------------------: |\n",
    "| $x_{1}$  | $\\dfrac{\\pi}{6}$                       | $1$                | $\\left(\\dfrac{\\pi}{6}, 1\\right)$ |\n",
    "| $x_{2}$  | $2x_{1}$                               | $2\\dot{x}_{1}$     | $\\left(\\dfrac{\\pi}{3}, 2\\right)$ |\n",
    "| $x_{3}$  | $\\sin(x_{2})$               | $\\cos\\left(x_{2}\\right)\\dot{x}_{2}$            | $\\left(\\dfrac{\\sqrt{3}}{2}, 1\\right)$ |\n",
    "| $x_{4}$  | $x_{3}^{2}$                            | $2x_{3}\\dot{x}_{3}$                   | $\\left(\\dfrac{3}{4}, \\sqrt{3}\\right)$ |\n",
    "\n",
    "By evaluating the derivative at each step of the chain rule, we eventually obtain the value of the derivative $f'(x) = \\sqrt{3}$ at $x = \\dfrac{\\pi}{6}$, as second entry of the final tuple in the table.\n",
    "\n",
    "While the above illustrates the forward mode of AD (the focus of our package), AD also has a reverse mode. Without using chain rule, it first does a forward pass to store the partial derivatives, before undertaking a reverse pass, which starts from the final function to be differentiated, $y$. After fixing the derivative of the final function, it then computes the derivative of each component function with respect to its parent function recursively (using chain rule) until the derivative of the function with respect to the basic-level argument (e.g. $x_1$) can be calculated. \n",
    "\n",
    "In terms of efficiency, the forward mode is more efficient when the number of functions to evaluate is much greater than the number of inputs, whereas the reverse mode, which computes the Jacobian-transpose-vector-product is more efficient when the number of inputs is much greater than the number of functions.\n",
    "\n",
    "More details on the reverse mode is covered in the **Extension (Reverse Mode)** section further below in the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to install `ADKit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ADKit` has only `numpy`(v. 1.14.3 or higher) as a pre-installation requirement, which we have included in a `requirements.txt` file in this repository. Following that, `ADKit` may be installed through the Python Package Index like so:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pip install ADKit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, the user may install `ADKit` by cloning the github repository (https://github.com/the-differentiators/cs207-FinalProject.git) or downloading as a zipped archive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once installed, the `AutoDiff` module can be imported simply through the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ADKit import AutoDiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the user chooses to clone the Github repository, once the package's Github repo has been downloaded, the `AutoDiff.py` module containing the classes can be imported by a Python file, with its working directory configured to the same directory as `AutoDiff.py`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use `ADKit` (Forward Mode)\n",
    "\n",
    "For the purposes of this demo, we will also import `numpy`, which is a requirement for the `ADKit` package, as well as the `Ad_Var` class from the `AutoDiff` module in the `ADKit` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ADKit.AutoDiff import Ad_Var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `ADKit` to compute derivative of a scalar function of one variable (forward mode)\n",
    "\n",
    "Below, we have included a basic demo for a scalar function, given a single input. The function used in the demo is  $f = sin^2(2x)$, which was used for illustration in the *Background* section earlier. Our objective is to use the the `Ad_Var` class to compute the value of the derivative for this function automatically, unlike the manual computational trace drawn out earlier. \n",
    "\n",
    "First, we create an instance of the `Ad_Var` object, with the value of $x = \\dfrac{\\pi}{6}$ assigned to the input variable, `val`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.pi / 6\n",
    "\n",
    "x = Ad_Var(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user should note that the `ADKit` package assumes that for a single input, the object being initialised will have a derivative value of 1 (stored as a Class attribute `self._ders`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create `f`, which represents the full function. The `Ad_Var` object from the previous code can be used with dunder functions and additional functions within `Ad_Var` class to construct the full function being evaluated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = (Ad_Var.sin(2*x))**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the functions are applied to the original `Ad_Var` object `x`, the `_val` and `_ders` attributes of the object are being updated with new values. The object `f`, representing the full function, will have its `_val` and `_ders` attributes containing the actual function and derivative values respectively.\n",
    "\n",
    "To note: the user also has the ability to manually set function and derivative values outside of instance initialization using the setter methods provided (`set_val` and `set_ders`). In this way, the user has the option to reuse the same objects after resetting the value and derivative(s).\n",
    "\n",
    "The associated function value and derivative(s) of any `Ad_Var` instance may be retrieved through the `get_val` and `get_ders` functions as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7499999999999999 1.7320508075688776\n"
     ]
    }
   ],
   "source": [
    "print(f.get_val(), f.get_ders())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the function value and derivative can be printed by directly printing the `Ad_Var` object associated with the function `f`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value = 0.7499999999999999\n",
      "Derivative = 1.7320508075688776\n"
     ]
    }
   ],
   "source": [
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `ADKit` to compute the gradient of a scalar multivariate function (forward mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the user wants to calculate the value and the gradient vector of a scalar multivariate function, then each variable must be first instantiated as an `Ad_Var` object, with inputs `val`, the scalar value of that variable, and `ders`, a `numpy` array representing the seed vector which indicates a direction along which the directional derivative of a function will be calculated. An example is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Ad_Var(1, np.array([1, 0, 0]))\n",
    "y = Ad_Var(2, np.array([0, 1, 0]))\n",
    "z = Ad_Var(3, np.array([0, 0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the user can define the function which consists of the instantiated `Ad_Var` variables. For example, below , we are calculating the value and the gradient of the function $f = sin^2(2x) + z^y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value = 9.826821810431806\n",
      "Gradient = [-1.51360499  9.8875106   6.        ]\n"
     ]
    }
   ],
   "source": [
    "f = (Ad_Var.sin(2*x))**2 + z**y\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the gradient of the function `f` is a 3-dimensional vector, since `f` is a function of 3 variables. The first dimension of the gradient vector is the directional derivative of `f` along the seed vector $[1, 0, 0]$. Since, `x` was instantiated with this seed vector, the first dimension of the gradient vector corresponds to the partial derivative $\\frac{\\partial f}{\\partial x}$ evaluated at $x=1, y=2, z=3$. Similarly, $y$ was instantiated with the seed vector $[0, 1, 0]$. In this way, the user has indicated that the second element of the gradient of `f` corresponds to $\\frac{\\partial f}{\\partial y}$ evaluated at at $x=1, y=2, z=3$. Similarly, $z$ was instantiated with the seed vector $[0, 0, 1]$, hence the third dimension of the gradient vector corresponds to $\\frac{\\partial f}{\\partial z}$ evaluated at $x=1, y=2, z=3$.\n",
    "\n",
    "In summary, each variable should be instantiated with a seed vector with dimensions equal to the dimensions of the gradient vector of the target function. For each variable, the values of the seed vector should be 0 except for one value which should be the derivative of that variable such as 1. The index of the element in the seed vector which has nonzero value indicates the index of the gradient vector which stores the partial derivative value of the target function with respect to this specific variable.  For example, if a variable is initiated with a seed vector of $[1,0,0]$, then this should be interpreted as the first variable among the three variables, and its derivative is set to be 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `ADKit` to compute derivative of a vector-valued multivariate function (forward mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user can also use `ADKit` to calculate the value and the jacobian matrix of a vector-valued function. Again the variables must be instantiated in the same way as discussed above. Then, a vector-valued function can be defined as a numpy array of functions composed of instantiated `Ad_Var` variables. An example is shown below for the vector valued function $f = \\begin{bmatrix}\n",
    "sin^2(2x) + z^y \\\\\n",
    "e^x + z\n",
    "\\end{bmatrix}$ for $x = 1, y = 2, z = 3$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Ad_Var(1, np.array([1, 0, 0]))\n",
    "y = Ad_Var(2, np.array([0, 1, 0]))\n",
    "z = Ad_Var(3, np.array([0, 0, 1]))\n",
    "\n",
    "f = np.array([(Ad_Var.sin(2*x))**2 + z**y, Ad_Var.exp(x) + z])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the user can call `get_jacobian` to get the jacobian matrix of `f` evaluated at $x = 1, y = 2, z = 3$. The first argument of this method is the vector-valued function $f$ defined as a numpy array. The second argument is the dimension of the vector of the functions (in this example the vector-valued function has 2 dimensions). The third argument is the number of variables composing the vector-valued function (in this example vector-valued function is composed of 3 variables, $x,y$ and $z$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.51360499,  9.8875106 ,  6.        ],\n",
       "       [ 2.71828183,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ad_Var.get_jacobian(f, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the user can call `get_values` by passing `f`, to calculate the value of the vector-valued function for the given values of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.82682181, 5.71828183])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ad_Var.get_values(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, the vector valued function can also be defined as a numpy array of other already instantiated functions, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = (Ad_Var.sin(2*x))**2 + z**y\n",
    "h = Ad_Var.exp(x) + z\n",
    "f = np.array([g, h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.51360499,  9.8875106 ,  6.        ],\n",
       "       [ 2.71828183,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ad_Var.get_jacobian(f, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.82682181, 5.71828183])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ad_Var.get_values(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `ADKit` to compute the derivatives of any type of function on a grid of points (forward mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above examples, the derivative/gradient/jacobian of a function is evaluated at a single point which is defined by the value with which each variable is instantiated. The `AutoDiff` module, however, can be used to evaluate the derivative/gradient/jacobian of a function on a grid of points defined by the user. The first step to do this is again to instantiate the variables with any value (please note that the default value of an `Ad_Var` variable is 1 so the value argument can be skipped)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Ad_Var(ders = np.array([1, 0, 0]))\n",
    "y = Ad_Var(ders = np.array([0, 1, 0]))\n",
    "z = Ad_Var(ders = np.array([0, 0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the user needs to define the function as a string using the same standard syntax used in any of the examples above. For example, if function is $f = sin^2(2x) + z^y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_string = \"(Ad_Var.sin(2*x))**2 + z**y\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the user can call `grid_eval` to calculate the gradient and the value of the given function on a grid of points. The first argument passed is the function string. The second argument is a list of strings where each string represents one of the variables used in the function string. The third argument is the list of the already instantiated `Ad_Var` objects which are referenced in the function string. The last argument is a list of lists defining the grid of all possible points that the user wants to calculate the gradient and the value of the function for. For example, below the function and its gradient are evaluated for all possible combinations of $(x, y, z)$ where $x \\in \\{1, 2\\}, y \\in \\{2, 3\\}, z=4$. The function returns a dictionary where each key is one of the points of the grid and the value is a tuple. The first element of the tuple is the value of the function at this point and the second element of the tuple is the gradient of the function evaluated at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 2, 4): (16.826821810431806,\n",
       "  array([-1.51360499, 22.18070978,  8.        ])),\n",
       " (1, 3, 4): (64.82682181043181,\n",
       "  array([-1.51360499, 88.72283911, 48.        ])),\n",
       " (2, 2, 4): (16.57275001690431,\n",
       "  array([ 1.97871649, 22.18070978,  8.        ])),\n",
       " (2, 3, 4): (64.57275001690431,\n",
       "  array([ 1.97871649, 88.72283911, 48.        ]))}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ad_Var.grid_eval(f_string, ['x', 'y', 'z'], [x, y, z], [[1, 2], [2,3], [4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `grid_eval` can also be used to evaluate the jacobian for vector valued functions at different points. In this case, the string representation of the vector-valued function must be written as a list of functions referencing the already instantiated `Ad_Var` variables. Please note that in this case the string representation corresponds to a list of functions and not a numpy array of functions. For example, if the user wants to evaluate the jacobian of the vector-valued function $f = \\begin{bmatrix}\n",
    "sin^2(2x) + z^y \\\\\n",
    "e^x + z\n",
    "\\end{bmatrix}$ at different points, the function string should be defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_string = \"[(Ad_Var.sin(2*x))**2 + z**y, Ad_Var.exp(x) + z]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, by calling `grid_eval` on this function string, a dictionary is returned where each key is one of the points of the grid and the value is a tuple. The first element of the tuple is the value of the function at this point and the second element of the tuple is the jacobian of the vector-valued function evaluated at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 2, 4): (array([16.82682181,  6.71828183]),\n",
       "  array([[-1.51360499, 22.18070978,  8.        ],\n",
       "         [ 2.71828183,  0.        ,  1.        ]])),\n",
       " (1, 3, 4): (array([64.82682181,  6.71828183]),\n",
       "  array([[-1.51360499, 88.72283911, 48.        ],\n",
       "         [ 2.71828183,  0.        ,  1.        ]])),\n",
       " (2, 2, 4): (array([16.57275002, 11.3890561 ]),\n",
       "  array([[ 1.97871649, 22.18070978,  8.        ],\n",
       "         [ 7.3890561 ,  0.        ,  1.        ]])),\n",
       " (2, 3, 4): (array([64.57275002, 11.3890561 ]),\n",
       "  array([[ 1.97871649, 88.72283911, 48.        ],\n",
       "         [ 7.3890561 ,  0.        ,  1.        ]]))}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ad_Var.grid_eval(f_string, ['x', 'y', 'z'], [x, y, z], [[1, 2], [2,3], [4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use `ADKit` (Reverse Mode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of the extension to the minimum requirements, we have implemented the reverse mode of Automatic Differentiation. Using a separate class `rAd_Var`, the user is able to implement the reverse mode using the `ADKit` package. The value of using the reverse mode over the forward mode is the increase in efficiency when the number of inputs is much greater than the number of functions.\n",
    "\n",
    "The user should note that usage of the `rAd_Var` class differs from that of the `Ad_Var` class in the following ways:\n",
    "* The initialization of a `rAd_Var` instance does not allow for the input of a derivative value. The implementation necessitates that the derivative of the instance is initialized as None. There is the option for the user to manually set the derivative using the `set_ders` function, if they so wish to.\n",
    "\n",
    "\n",
    "* The derivatives of the `rAd_Var` object obtained using the `get_ders` method will be returned as a numpy array of partial derivative(s) of the input variables, unlike the forward mode, where the final derivative at the given value of the input variables is calculated using the chain rule.\n",
    "\n",
    "\n",
    "* To obtain a Jacobian matrix for vector functions with multiple real scalar inputs, e.g. $f = \\begin{bmatrix}\n",
    "xy \\\\\n",
    "y \\\\\n",
    "ln(x^y)\\\\\n",
    "\\end{bmatrix}$, the user will need to define the functions first before passing them (as Python functions) as arguments for the `get_jacobian` method, together with the variable names and the given values for the inputs, as demonstrated below. \n",
    "\n",
    "*Note: In defining the functions, the user should only include variables used in the function as arguments. Adding additional variables would lead to errors in the calculation.*\n",
    "\n",
    "This difference in the implementation of the Jacobian matrix is because `rAd_Var` objects can only be defined in the context of one function. Hence, feeding all the functions into the `get_jacobian` method allows for the Jacobian to obtain the respective partial derivatives for each function separately, before combining them and returning them in a single Jacobian matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `ADKit` to compute derivative of a scalar function of one variable (reverse mode)\n",
    "\n",
    "As a demo, we will again find the value and the derivative of $f = sin^2(2x)$ at $x = \\dfrac{\\pi}{6}$, similar to the demo for `Ad_Var` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ADKit.AutoDiff import rAd_Var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value = 0.7499999999999999\n",
      "Partial Derivative(s) = [1.73205081]\n"
     ]
    }
   ],
   "source": [
    "a = np.pi / 6\n",
    "\n",
    "x = rAd_Var(a)\n",
    "\n",
    "f = (rAd_Var.sin(2*x))**2\n",
    "\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `rAd_Var` to compute derivative, values of a vector-valued multivariate function (reverse mode)\n",
    "\n",
    "We will use a similar function as shown above to obtain the Jacobian matrix of a vector-valued function. The functions in the vector must be defined as Python functions as discussed above. A basic example is shown below for the vector valued function $f = \\begin{bmatrix}\n",
    "xy \\\\\n",
    "y \\\\\n",
    "ln(x^y)\\\\\n",
    "\\end{bmatrix}$ at the points where $x = 1, y = 2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 1.],\n",
       "       [0., 1.],\n",
       "       [2., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f1(x, y):\n",
    "    return x * y\n",
    "\n",
    "def f2(y):\n",
    "    return y\n",
    "\n",
    "def f3(x, y):\n",
    "    return rAd_Var.log(x ** y)\n",
    "\n",
    "rAd_Var.get_jacobian([f1, f2, f3], [\"x\",\"y\"], [1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the event that more variables are defined in the constructor but not used in the functions, their partial derivative is defined in the Jacobian matrix as 0, as shown below with the inclusion of the variable `z`. This gives the user flexibility in adjusting the vector functions to include/exclude a variable where needed. Note: In the arguments passed into the Python functions, the user should only include variables used in the function as arguments (e.g. for `f2`, including only `y` and not both `x` and `y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [2., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f1(x, y):\n",
    "    return x * y\n",
    "\n",
    "def f2(y):\n",
    "    return y\n",
    "\n",
    "def f3(x, y):\n",
    "    return rAd_Var.log(x ** y)\n",
    "\n",
    "rAd_Var.get_jacobian([f1, f2, f3], [\"x\",\"y\",\"z\"], [1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_values` method is then used to return an array of computed values for the functions passed into the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 0.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rAd_Var.get_values([f1, f2, f3], [\"x\",\"y\",\"z\"], [1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of `rAd_Var` vs. `Ad_var` in obtaining the Jacobian matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the implementation of the Jacobian matrix for the reverse mode is different, it will return a similar result as the forward mode (implemented in the `Ad_Var` class). We recommend that the user uses the `rAd_Var` class, in the event that the number of inputs is significantly greater than the number of functions, where it will perform more efficiently.\n",
    "\n",
    "Below we compare the output from the `get_jacobian` method from both methods, based on the more complicated equation $f = \\begin{bmatrix}\n",
    "sin^2(2x) + z^y \\\\\n",
    "e^x + z\n",
    "\\end{bmatrix}$ for $x = 1, y = 2, z = 3$, used in the earlier demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.51360499,  9.8875106 ,  6.        ],\n",
       "       [ 2.71828183,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Ad_Var(1, np.array([1, 0, 0]))\n",
    "y = Ad_Var(2, np.array([0, 1, 0]))\n",
    "z = Ad_Var(3, np.array([0, 0, 1]))\n",
    "\n",
    "f = np.array([(Ad_Var.sin(2*x))**2 + z**y, Ad_Var.exp(x) + z])\n",
    "Ad_Var.get_jacobian(f, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.51360499,  9.8875106 ,  6.        ],\n",
       "       [ 2.71828183,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f1(x, z, y):\n",
    "    return rAd_Var.sin(2*x)**2 + z**y\n",
    "\n",
    "def f2(x, z):\n",
    "    return rAd_Var.exp(x) + z\n",
    "\n",
    "rAd_Var.get_jacobian([f1, f2], [\"x\",\"y\",\"z\"], [1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory structure\n",
    "               \n",
    "Our intended directory structure is as follows:\n",
    "```\n",
    "cs207-FinalProject/\n",
    "                   ADKit/\n",
    "                        test/\n",
    "                            test_autodiff.py\n",
    "                            test_autodiff_reverse.py\n",
    "                        AutoDiff.py\n",
    "                   demo/\n",
    "                        demo.py               \n",
    "                   docs/\n",
    "                        documentation.ipynb\n",
    "                   LICENSE\n",
    "                   README.md\n",
    "                   requirements.txt\n",
    "                   setup.cfg\n",
    "                   setup.py\n",
    "```                 \n",
    "\n",
    "### Modules\n",
    "\n",
    "The primary module is a single `AutoDiff.py` file. Contained within it are two classes - the `Ad_Var` class and `rAd_Var` class. \n",
    "\n",
    "Instances of these two classes, through interaction with other objects of the same class, are able to compute the value of a function as well as the value of that function's derivative with respect to any input variable. This module is powerful enough to handle both forward and reverse mode of Automatic Differentiation of any function comprised of the following elementary functions:\n",
    "\n",
    "* Fundamental arithmetic operators (addition, subtraction, multiplication, and division)\n",
    "* Logarithm (of any base)\n",
    "* Negation\n",
    "* Exponentiation ($e^x$ for an `Ad_Var` instance $x$)\n",
    "* Power and root functions ($x^n$ for some real $n$)\n",
    "* Trigonometric functions ($\\sin(x)$, $\\cos(x)$, $\\tan(x)$)\n",
    "* Inverse trigonometric functions ($\\arcsin(x)$, $\\arccos(x)$, $\\arctan(x)$)\n",
    "\n",
    "Each instance of the `Ad_Var` and `rAd_Var` class in the `AutoDiff` module in the `ADKit` package represents the definition of a set of variables at a particular evaluation point. Through manipulations of these instances (either through fundamental arithmetic operations or built-in methods representing additional elementary functions described earlier), a user has the capability of representing any continuous differentiable function, be it scalar or vector. This was shown earlier via the code demo.\n",
    "\n",
    "The other modules in the package are stored in the `test` folder and make up the test-suite for `AutoDiff.py`, with more details in the *Testing and Coverage* section below.\n",
    "\n",
    "### Testing and Coverage\n",
    "In the `test` folder in the `ADKit` package, there are two separate Python modules `test_autodiff.py` and `test_autodiff_reverse.py`, which together consist of the test-suite for the `AutoDiff` module.\n",
    "\n",
    "`test_autodiff.py` will contain tests for the methods in the `Ad_Var` class and `test_autodiff_reverse.py` for the `rAd_Var` class, to ensure that the elementary functions return the desired output. Tests are run using pytest. The tests are linked to Travis CI and CodeCov, which will manage continuous integration and code coverage respectively.\n",
    "\n",
    "###  Installation and Distribution of package \n",
    "The package is distributed via PyPI. There is no additional packaging framework included; we believe the scope of this project can be contained within a relatively simple directory structure with few functional python files and should not require additional overhead for users to install and use.\n",
    "\n",
    "The user is able to install the package in the standard way using `pip`, as shown above in the *How to Use* section. We have also provided a `requirements.txt` file which can be used by the user to create the appropriate environment and ensure that dependencies (i.e. `numpy`) have been installed. Then, to use the package, the user will only need to import `ADKit`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Details\n",
    "\n",
    "### Class Implementation and Core Attributes\n",
    "\n",
    "* There are two classes used for forward mode, as well as the extension (reverse mode): `Ad_Var` class and `rAd_Var` class respectively.\n",
    "\n",
    "* The choice of keeping them as two separate classes is based on the fact that there is limited resuability of code between both implementations - the forward mode determines the derivatives of the variables using the chain rule whereas reverse mode traverses the computational graph in the forward pass and stores both parent-child relationships and the respective partial derivatives of the variables without doing the chain rule.\n",
    "\n",
    "#### `Ad_Var` Class (Reverse Mode)\n",
    "\n",
    "* The `Ad_Var`  class will represent the variables that are used in the forward mode of Automatic Differentiation process. In the case of a single input, the instance should be initialized with, `val`, a scalar value of that variable to be evaluated on when calculating both the function and derivative values (as shown in the demo above)\n",
    "\n",
    "* In the case of multiple inputs, each input is initialized as an `Ad_Var` object, with inputs `val`, a scalar value of that variable and `ders`, a `numpy` array representing the derivative of the input with regards to the other variables. An example is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = Ad_Var(1, np.array([1, 0, 0]))\n",
    "x2 = Ad_Var(2, np.array([0, 1, 0]))\n",
    "x3 = Ad_Var(3, np.array([0, 0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dunder methods such as \"add\" and \"mul\", and other elementary functions are implemented under this class. More information on this is covered below in the *Class Methods* section. \n",
    "\n",
    "* As part of the class methods, we have included two static methods, `get_jacobian` and `get_values`, which respectively compute the Jacobian matrix and an array of function values for an array of `Ad_Var` objects. Also, a static method `grid_eval` is included which evaluates the function and its derivative/gradient/jacobian on a grid of points.\n",
    "\n",
    "* In our implementation, we will also use the try-except method to catch unexpected input types: for example, if the user initializes the variable value of the `Ad_Var` instance with a value of type string, which is not a valid input type.\n",
    "\n",
    "\n",
    "#### `Ad_Var`: Core Attributes\n",
    "\n",
    "* `_val`: float value, indicating the function value of the Ad_Var object evaluated at the given point\n",
    "* `_ders` (for single input): float value, indicating the derivative value of Ad_Var object evaluated at the given point\n",
    "* `_ders` (for multiple inputs): 1-D array of floats, representing the value of the derivatives of the multiple inputs evaluated at the given point\n",
    "\n",
    "`_val` and `_ders` attributes are made pseudoprivate to prevent users from manually setting function and derivative values outside of instance initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `rAd_Var` Class (Reverse Mode)\n",
    "\n",
    "* The `rAd_Var`  class will represent the variables that are used in the reverse mode of Automatic Differentiation process. In the case of a single input, the instance should be initialized with, `val`, a scalar value of that variable to be evaluated on when calculating both the function and derivative values (as shown in the demo above)\n",
    "\n",
    "* The initialization of a `rAd_Var` instance does not allow for the input of a derivative value. The implementation necessitates that the derivative of the instance is initialized as None. There is the option for the user to manually set the derivative using the `set_ders` function, if they so wish to.\n",
    "\n",
    "* The derivatives of the `rAd_Var` object obtained using the `get_ders` method are returned as a numpy array of partial derivative(s) of the input variables with respective to the final function\n",
    "\n",
    "* As part of the class methods, we have included two static methods, `get_jacobian` and `get_values`, which respectively compute the Jacobian matrix and an array of function values for an array of `rAd_Var` objects. \n",
    "\n",
    "* To obtain a Jacobian matrix for vector functions, the user will need to define the functions first before passing them (as Python functions) as arguments for the `get_jacobian` method, together with the variable names and the given values for the inputs, as shown in the demo above.\n",
    "\n",
    "* This difference in the implementation of the Jacobian matrix is because `rAd_Var` objects can only be defined in the context of one function. Hence, feeding all the functions into the `get_jacobian` method allows for the Jacobian to obtain the respective partial derivatives for each function separately, before combining them and returning them in a single Jacobian matrix.\n",
    "\n",
    "* In the case of multiple inputs, each input is initialized as an `rAd_Var` object, with inputs `val`.\n",
    "\n",
    "#### `rAd_Var`: Core Attributes\n",
    "\n",
    "* `_val`: float value, indicating the function value of the rAd_Var object evaluated at the given point\n",
    "* `_ders`: instantiated as None and updated via the `get_ders` method which sets the derivative of the final function (with respect to itself) as 1 before using the `get_gradient` helper method to recursively go through all children of the input variables and updating `_ders` with their partial derivative value   \n",
    "* `parents`: lists containing the parent node(s) of the rAd_Var object; initialized as an empty list and populated at every computation (via class methods)\n",
    "* `children`: lists of tuples containing the children node(s) of the rAd_Var object; initialized as an empty and populated at every computation (via class methods)\n",
    "* `visited`: boolean value used to track if a node has been traversed in the reverse pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Data Structures\n",
    "\n",
    "In both classes, the following core data structures were used:\n",
    "\n",
    "* **`numpy` arrays**: 1-D `numpy` arrays are used to keep the gradient vectors as the entire trace is evaluated. `numpy`\n",
    "provides vectorized operations which will make the overloading of elementary functions much more efficient for\n",
    "multivariate functions. If a vector function is provided, 2-D `numpy` arrays are used to hold the Jacobian matrix.\n",
    "\n",
    "* **Dictionaries**: In the `Ad_Var` class, dictionaries are used to keep the results of `grid_eval` function call. Particularly, the keys of the dictionary are points on the grid defined by the user and the corresponding values are the function value and its derivative/gradient/jacobian at the corresponding point. In the `rAd_Var` class, dictionaries are used extensively in the `get_jacobian` method to keep and track the variable inputs and the partial derivatives for each variable separately.\n",
    "\n",
    "Specifically, in the `rAd_Var` class, where a node is defined as the point at which the computation of a new `rAd_Var` instance is performed, lists are used to store the the parent-child relationships of each node.\n",
    "\n",
    "* **Lists**: At the every computation of a new `rAd_Var` instance, the new object and the partial derivative of the new function with respect to the input variable are stored as a tuple. This simulates the forward pass of the reverse mode. The tuples in the list are then accessed in the reverse pass of the reverse mode using the `get_ders` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External Dependencies\n",
    "* `numpy` for implementation of the elementary functions (e.g. sin, sqrt, log and exp), by overloading `numpy` implementations for these functions\n",
    "* `pytest` and `doctest` for testing\n",
    "* TravisCI and CodeCov used to manage continuous integration and code coverage\n",
    "\n",
    "### Elementary Functions and Class Methods\n",
    "\n",
    "While both the forward mode and reverse mode support a same set of basic operations, comparison operators and elementary functions, the computation of the attributes of the object returned by the functions is vastly different.\n",
    "\n",
    "At each computation, the reverse mode stores both parent-child relationships and the respective partial derivatives of the variables to each other without doing the chain rule via the attributes `parents` and `children`, covered above. Below are the functions supported by both classes - for more details on how the partial derivatives of the variables are calculated for `rAd_Var`, refer to the code.\n",
    "\n",
    "#### Elementary Functions / Operators supported by both classes\n",
    "* `__add__(self, other)` and `__radd__(self, other)`:\n",
    "    * Other can be a float, int or an `AutoDiff` object\n",
    "    * Returns an `Ad_Var` or `rAd_Var` object when calculating self + other or other + self\n",
    "\n",
    "\n",
    "* `__sub__(self, other)` and `__rsub__(self, other)`:\n",
    "    * Other can be a float, int or an `AutoDiff` object\n",
    "    * Returns an `Ad_Var` or `rAd_Var` object when calculating self - other or other - self\n",
    "\n",
    "\n",
    "* `__mul__(self, other)` and `__rmul__(self, other)`:\n",
    "    * Other can be a float, int or an `AutoDiff` object\n",
    "    * Returns an `Ad_Var` or `rAd_Var` object when calculating self * other or other * self\n",
    "    \n",
    "\n",
    "* `__truediv__(self, other)` and `__rtruediv__(self, other)`:\n",
    "    * Other can be a float, int or an `AutoDiff` object\n",
    "    * Returns an `Ad_Var` or `rAd_Var` object when calculating self / other or other / self\n",
    "\n",
    "\n",
    "* `__pow__(self, other)` and `__rpow__(self, other)`:\n",
    "    * `other` can be a float, int or an `AutoDiff` object\n",
    "    * `__rpow__` will require `other` to be a numeric type, otherwise, it will raise a TypeError\n",
    "    * Returns an `Ad_Var` or `rAd_Var` object when calculating self ** other\n",
    "\n",
    "\n",
    "* `__neg__(self)`:\n",
    "    * Returns an `Ad_Var` or `rAd_Var` object when calculating - self\n",
    "\n",
    "\n",
    "* `__eq__(self, other)`:\n",
    "    * Returns True if `self._val` == `other._val` and `self._ders` == `other._ders`, returns False otherwise\n",
    "    \n",
    "    \n",
    "* `__ne__(self, other)`:\n",
    "    * Returns True if `self._val` != `other._val` or `self._ders` != `other._ders`, returns False otherwise\n",
    "    \n",
    "    \n",
    "* `__repr__(self)`:\n",
    "    * Returns a string representing the value of `self._val` (Value) and the value of `self._ders` (Gradient)\n",
    "    \n",
    "\n",
    "*  `sqrt(self)`:\n",
    "    * Returns an `Ad_Var` or `rAd_Var` object by calling the __pow__ method using self**0.5\n",
    "    \n",
    "\n",
    "* `exp(self)`:\n",
    "    * `Ad_Var`: Returns an `Ad_Var` object with `self._val = np.exp(self._val)` and `self._ders = np.exp(self._val) * self._ders`\n",
    "    * `rAd_Var`: Returns an `rAd_Var` object with `self._val = np.exp(self._val)` and the `parent` and `children` attributes for both `self` and the new `rAd_Var` object updated accordingly\n",
    "\n",
    "\n",
    "* `log(self, logbase=np.e)`:\n",
    "    * Optional argument for `logbase` (can be a float or int). By default, `logbase` is set to the exponential.\n",
    "    * `Ad_Var`: Returns an `Ad_Var` object with `self._val = np.log(self._val) / np.log(logbase)` and `self._ders = self._ders / (self._val * np.log(logbase))`.\n",
    "    * `rAd_Var`: Returns an `rAd_Var` object with `self._val = np.log(self._val) / np.log(logbase)` and the `parent` and `children` attributes for both `self` and the new `rAd_Var` object updated accordingly\n",
    "    \n",
    "    \n",
    "* `sin(self)` and `cos(self)` and `tan(self)`:\n",
    "    * Returns an `Ad_Var` or `rAd_Var` object with the respective class attributes updated accordingly based on the given trigonometric function\n",
    "    \n",
    "    \n",
    "* `arcsin(self)` and `arccos(self)` and `arctan(self)`:\n",
    "    * Returns an `Ad_Var` or `rAd_Var` object with respective class attributes updated accordingly based on the given inverse trigonometric function  \n",
    "\n",
    "\n",
    "* `sinh(self)` and `cosh(self)` and `tanh(self)`:\n",
    "    * Returns an `Ad_Var` or `rAd_Var` object with respective class attributes updated accordingly based on the given hyperbolic function\n",
    "\n",
    "\n",
    "* `logistic(self)`: \n",
    "    * Returns an `Ad_Var` or `rAd_Var` object with respective class attributes updated accordingly based on the logistic (sigmoid) function\n",
    "    \n",
    "\n",
    "* `set_val(self, value)`:\n",
    "    * Set the value of the private attribute `self._val` with `value`\n",
    "\n",
    "\n",
    "* `set_ders(self, derivatives)`:\n",
    "    * Set the value of the private attribute `self._ders` with `derivatives`\n",
    "    \n",
    "    \n",
    "* `get_val(self)`:\n",
    "    * Returns the value of the attribute `self._val` \n",
    "    \n",
    "    \n",
    "#### Specific `Ad_Var` class methods\n",
    "* `__init__(self, val, ders=1)`:\n",
    "    * Sets `self._val` to the argument `val`\n",
    "    * Sets `self._ders` to the argument `ders`\n",
    "  \n",
    "  \n",
    "* `get_ders(self)`:\n",
    "    * Returns the value of the attribute `self._ders` \n",
    "    \n",
    "    \n",
    "* `get_jacobian(functions_array, functions_dim, vars_dim)`:\n",
    "    * Static method that returns the Jacobian matrix for a given array of `Ad_Var` objects \n",
    "    \n",
    "    \n",
    "* `get_values(functions_array)`:\n",
    "    * Static method that returns an array of function values for a given array of `Ad_Var` objects\n",
    "   \n",
    "   \n",
    "* `grid_eval(func_string, vars_strings, Ad_Vars_list, grid)`:\n",
    "    * Static method that evaluates a function and its derivative/gradient/jacobian on a grid of points. A dictionary is returned where each key is a point of the grid and the value is a tuple with the first element being the value of the function at this point and second element is the derivative/gradient/jacobian evaluated at this point.\n",
    "\n",
    "\n",
    "#### Specific `rAd_Var` class methods\n",
    "* `__init__(self, val, ders=1)`:\n",
    "    * Sets `self._val` to the argument `val`\n",
    "    * Initializes `self._ders` as `None`\n",
    "    * Initializes `self.parents` as an empty list\n",
    "    * Initializes `self.children` as an empty list\n",
    "    * Initializes `self.visited` as `False`\n",
    "    \n",
    "\n",
    "* `get_ders(self)`:\n",
    "    * Sets the derivative of the final function (with respect to itself) as 1 before using the `get_gradient` helper method, which recursively traverses all children of the input variables and updating `_ders` with their partial derivative value   \n",
    "    * Returns `gradient_matrix`, a `numpy` array consisting of the partial derivatives of input variables used to compute the final function\n",
    "    \n",
    "    \n",
    "* `get_jacobian(functions_array, var_list, var_values)`:\n",
    "    * Static method that returns the Jacobian matrix for a vector of Python functions, with given variable names and values for the variables used as arguments in these functions\n",
    "    * Instantiation of `rAd_Var` objects is done within this method based on the functions and variables passed in\n",
    "    \n",
    "    \n",
    "* `get_values(functions_array, var_list, var_values)`:\n",
    "    * Static method that returns an array of function values for a vector of Python functions, with given variable names and values for the variables used as arguments in these functions\n",
    "    * Instantiation of `rAd_Var` objects is done within this method based on the functions and variables passed in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension (Reverse Mode)\n",
    "\n",
    "### Description \n",
    "\n",
    "As part of an extension from the earlier milestone, we have implemented the reverse mode of Automatic Differentiation using the `rAd_Var` class, with details on the class (data structures, attributes and methods) elaborated and demonstrated above.\n",
    "\n",
    "In terms of efficiency, the forward mode is more efficient when the number of functions to evaluate is much greater than the number of inputs, whereas the reverse mode, which computes the Jacobian-transpose-vector-product is more efficient when the number of inputs is much greater than the number of functions.\n",
    "\n",
    "Having the `rAd_Var` class allows the user of this package the flexibility to choose between the two modes of automatic differentiation, depending on the vector functions and variables that they will carry out Automatic Differentiation on.\n",
    "\n",
    "### Additional information / background\n",
    "\n",
    "Unlike the forward mode of Automatic Differentiation, the reverse mode of Automatic Differentiation consists of two stages - 1) forward pass, followed by the 2) reverse pass (also popularly known as backward propagation).\n",
    "\n",
    "With a function $f$ which takes in $n$ inputs, $x_1, x_2, ..., x_n$ and produces a single output, the reverse mode will return derivatives of the function $\\dfrac{\\partial f}{\\partial x_{i}}$ for all $i$.\n",
    "\n",
    "The following references and builds on [CS207 2019 Lecture 12 materials](https://harvard-iacs.github.io/2019-CS207/lectures/lecture12/notebook/) (Sondak):\n",
    "\n",
    "**Forward pass**\n",
    "\n",
    "In the forward pass, the evaluation of the function is carried out at each elementary function, $f_i$, of the entire function, $f$. What this means is that for $i = n + 1, n + 2, ..., N$, where $\\pi(i)$ denotes the set of \"parents\" of $x_i$.\n",
    "\n",
    "$$x_i = f_i(x_\\pi(i))$$ \n",
    "\n",
    "E.g. for $x_{3} = x_{1}x_{2}$, where the elementary function, $f_3$, is a multiplication of two input variables, thus forming a node in the computational graph. Here, $\\pi(3) = (x_1, x_2)$. This is important in the forward pass, which stores the partial derivatives of every node with respect to its \"parents\", as denoted by $\\pi(i)$. In the example above, where $x_{3} = x_{1}x_{2}$, we store $\\dfrac{\\partial x_{3}}{\\partial x_{1}}$ and $\\dfrac{\\partial x_{3}}{\\partial x_{2}}$.\n",
    "\n",
    "This can be best illustrated using an example where $$ f (x, y) = 2xy - \\exp(xy)$$ at the point $x = 1, y = 2$\n",
    "\n",
    "Here, to generate the forward trace, we calculate the partial derivatives of a node with regards to its children:\n",
    "\n",
    "| Node   | Current Value      | Numerical Value           | $\\partial_{1}$       | $\\partial_{1}$ Value  | $\\partial_{2}$ | $\\partial_{2}$ Value |\n",
    "| :---: | :----------------------------: | :-----------: | :----------------------------: | :-----------------:  | :-----------------: | :-----------------: |\n",
    "| $x_{1}$ | $x$                      | $1$         | $1$                        | $1$     | $0$     | $0$   |\n",
    "| $x_{2}$ | $y$                      | $2$         | $0$                        | $0$     | $1$     | $1$   | \n",
    "| $x_{3}$ | $x_{1}x_{2}$             | $2$         | $x_{2}$                    | $2$     | $x_{1}$ | $1$   |\n",
    "| $x_{4}$ | $2x_{3}$                 | $4$         | $2$                        | $2$     | $-$     | $-$   |\n",
    "| $x_{5}$ | $\\exp\\left(x_{3}\\right)$ | $e^{2}$     | $\\exp\\left(x_{3}\\right)$   | $e^{2}$ | $-$     | $-$   |\n",
    "| $x_{6}$ | $-x_5$                   | $-e^{2}$    | $-1$                       | $-1$    | $-$     | $-$   |\n",
    "| $x_{7}$ | $x_{4} + x_{6}$          | $4 - e^{2}$ | $1$                        | $1$     | $1$     | $1$   |\n",
    "\n",
    "**Reverse pass**\n",
    "\n",
    "Following the forward pass, the reverse pass starts from the final function to be differentiated, $f_N$, setting $\\overline{x}_{N} = \\dfrac{\\partial f}{\\partial x_{N}} = 1$ (since $f = x_{N}$).\n",
    "\n",
    "Then, using the chain rule, it traverses the computational graph to obtain values for the partial derivative of every variable in the computational graph, $x_i$: \n",
    "\n",
    "\\begin{align}\n",
    "  \\overline{x}_{i} = \\dfrac{\\partial f}{\\partial x_{i}} = \\sum_{\\text{j a child of i}}{\\dfrac{\\partial f}{\\partial x_{j}}\\dfrac{\\partial x_{j}}{\\partial x_{i}}}.\n",
    "\\end{align}\n",
    "\n",
    "This is done recursively until the partial derivatives of the function with respect to the $n$ inputs, $x_1, x_2, ..., x_n$ are computed.\n",
    "\n",
    "Using the same example above, we recursively go through every variable($x_i$) in the computational trace shown above. The computation of the gradient of each variable accesses the partial derivative with respect to its children that has been previously computed and stored during the forward pass.\n",
    "\n",
    "$$\\overline{x}_{7} = \\dfrac{\\partial f}{\\partial x_{7}} = 1$$\n",
    "\n",
    "$$\\overline{x}_{6} = \\dfrac{\\partial f}{\\partial x_{7}}\\dfrac{\\partial x_{6}}{\\partial x_{7}} = 1 \\cdot 1 = 1$$\n",
    "\n",
    "$$\\overline{x}_{5} = \\dfrac{\\partial f}{\\partial x_{6}}\\dfrac{\\partial x_{6}}{\\partial x_{5}} = 1 \\cdot (-1) = -1$$\n",
    "\n",
    "$$\\overline{x}_{4} = \\dfrac{\\partial f}{\\partial x_{7}}\\dfrac{\\partial x_{7}}{\\partial x_{4}} = 1 \\cdot 1 = 1$$\n",
    "\n",
    "$$\\overline{x}_{3} = \\dfrac{\\partial f}{\\partial x_{5}}\\dfrac{\\partial x_{5}}{\\partial x_{3}} + \\dfrac{\\partial f}{\\partial x_{4}}\\dfrac{\\partial x_{4}}{\\partial x_{3}}= (-1 \\cdot e^{2}) + (1\\cdot 2) = 2 - e^{2}$$\n",
    "\n",
    "$$\\overline{x}_{2} = \\dfrac{\\partial f}{\\partial x_{3}}\\dfrac{\\partial x_{3}}{\\partial x_{2}} = (2 - e^{2}) \\cdot 1 = 2 - e^{2}$$\n",
    "\n",
    "$$\\overline{x}_{1} = \\dfrac{\\partial f}{\\partial x_{3}}\\dfrac{\\partial x_{3}}{\\partial x_{1}} = (2 - e^{2}) \\cdot 2 = 4 - 2e^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the gradient of:\n",
    "\n",
    "\\begin{align}\n",
    "  \\nabla f &= \n",
    "  \\begin{bmatrix} \n",
    "    4 - 2e^{2} \\\\ \n",
    "    2 - e^{2} \n",
    "  \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "which is identical to what we would have calculated using symbolic differentiation for the function $f (x, y) = 2xy - \\exp(xy)$:\n",
    "\n",
    "$$\\nabla f = \\begin{bmatrix} 2y - \\exp\\left(xy\\right)y \\\\ 2x - \\exp\\left(xy\\right)x \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reverse mode of Automatic Differentiation is supported by the `rAd_Var` class in the `AutoDiff` module. Further details on using the class is covered in the *How to use `ADKit` (Reverse Mode)* section of the Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "* [A Hitchhikerâ€™s Guide to Automatic Differentiation](https://link.springer.com/article/10.1007/s11075-015-0067-6)\n",
    "* [A simple explanation of reverse-mode automatic differentiation](https://justindomke.wordpress.com/2009/03/24/a-simple-explanation-of-reverse-mode-automatic-differentiation/)\n",
    "* Harvard CS207 2019 course materials"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
