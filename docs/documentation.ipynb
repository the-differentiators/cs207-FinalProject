{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS207 Project Group 11 - \"The Differentiators\"\n",
    "# Milestone 2\n",
    "\n",
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Derivatives are ubiquitous in many fields such as engineering design optimization, fluid dynamics and machine learning.\n",
    "There are in general three ways to calculate the derivatives: automatic differentiation, numeric differentiation, and\n",
    "symbolic differentiation. Automatic Differentiation (AD) brings a family of techniques that can calculate the partial\n",
    "derivatives of any function at any point efficiently and accurately. Unlike numeric differentiation, AD does not have\n",
    "the problem of floating point precision errors, since it calculates the derivative of a simple function, and keeps track of\n",
    "these derivatives, and there is no need of step sizes. Compared to symbolic differentiation, AD is not as memory intense,\n",
    "and can be much faster in terms of the calculation. Therefore, AD is an important way to calculate derivatives in\n",
    "practice.\n",
    "\n",
    "There are two modes in Automatic Differentiation: forward mode and the reverse mode. In forward mode, the chain rule is applied to each basic operation, and both the variable's value and derivative are calculated along the way, leading to a complete derivative trace. In reverse mode, there is a forward phase, where the intermediate variables are computed and their values and partial derivatives with respect to the previous layer stored in the memory, and also a backward phase, where we propagate back the derivatives with the help of the chain rule.\n",
    "\n",
    "The software that we design calculates the derivatives given the user’s input using the forward mode/reverse mode of automatic differentiation depending on the user's choice,\n",
    "and provides the user with an easy way to solve their optimization problem using derivatives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "At the core of Automatic Differentiation is the principle that functions implemented as computer code can be broken down into elementary functions, ranging from arithmetic operations (e.g. addition, subtraction etc.) and other functions (e.g. power, exponential, sin etc.). Hence, any differentiable function can be interpreted as a composition of different functions. \n",
    "\n",
    "For example, given a function, $f = sin^2(2x)$, it can be rewritten as:\n",
    "\n",
    "$$ f = \\phi_1(\\phi_2(\\phi_3(x))) $$ \n",
    "\n",
    "where $$ \\phi_1(z) = z^2,   \\phi_2(y) = sin(y) \\text{ and } \\phi_3(x) = 2x$$\n",
    "\n",
    "\n",
    "In the forward mode, the chain rule can then be applied successively to each elementary component function to obtain the derivative of the function. Using the same example above, let $c$ be a real number:\n",
    "$$ f'(c) =  \\phi_3'(\\phi_2(\\phi_1(c))) \\cdot \\phi_2'(\\phi_1(c)) \\cdot \\phi_1'(c)$$\n",
    "\n",
    "Based on the example above, the derivative, $f'(c)$, can be evaluated based on the following function-derivative pairs at each stage of computing the function:\n",
    "\n",
    "$$(\\phi_1(c), \\phi_1'(c))$$\n",
    "\n",
    "$$(\\phi_2(\\phi_1(c)), (\\phi_2'(\\phi_1(c)) \\cdot \\phi_1'(c)))$$\n",
    "\n",
    "$$(\\phi_3(\\phi_2(\\phi_1(c))), \\phi_3'(\\phi_2(\\phi_1(c)) \\cdot \\phi_2'(\\phi_1(c)) \\cdot \\phi_1'(c))$$\n",
    "\n",
    "Effectively, the forward mode computes the Jacobian-vector product, $Jp$. This decomposition can be represented via a computational graph structure of calculations, requiring initial values to be set for $x_1$, and $x'_1$:\n",
    "\n",
    "$$x_1 \\rightarrow^{\\phi_3(x)} x_2 \\rightarrow^{\\phi_2(x)} x_3 \\rightarrow^{\\phi_1(x)} y $$\n",
    "\n",
    "where $$ \\phi_1(x) = x^2,   \\phi_2(x) = sin(x) \\text{ and } \\phi_3(x) = 2x$$\n",
    "\n",
    "At each stage of the function, the derivative of the function with respect to its argument is calculated. The exact values of the function and its derivative are used for the following function-derivative pair of values. An example of the computational trace for the equation $f = sin^2(2x)$ would look like this, for $x = \\dfrac{\\pi}{6}$. \n",
    "\n",
    "| Trace    | Elementary Operation &nbsp;&nbsp;&nbsp;| Derivative &nbsp;&nbsp;&nbsp; | $\\left(f\\left(a\\right),  f^{\\prime}\\left(a\\right)\\right)$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|\n",
    "| :------: | :----------------------:               | :------------------------------: | :------------------------------: |\n",
    "| $x_{1}$  | $\\dfrac{\\pi}{6}$                       | $1$                | $\\left(\\dfrac{\\pi}{6}, 1\\right)$ |\n",
    "| $x_{2}$  | $2x_{1}$                               | $2\\dot{x}_{1}$     | $\\left(\\dfrac{\\pi}{3}, 2\\right)$ |\n",
    "| $x_{3}$  | $\\sin(x_{2})$               | $\\cos\\left(x_{2}\\right)\\dot{x}_{2}$            | $\\left(\\dfrac{\\sqrt{3}}{2}, 1\\right)$ |\n",
    "| $x_{4}$  | $x_{3}^{2}$                            | $2x_{3}\\dot{x}_{3}$                   | $\\left(\\dfrac{3}{4}, \\sqrt{3}\\right)$ |\n",
    "\n",
    "By evaluating the derivative at each step of the chain rule, we eventually obtain the value of the derivative $f'(x) = \\sqrt{3}$ at $x = \\dfrac{\\pi}{6}$, as second entry of the final tuple in the table.\n",
    "\n",
    "While the above illustrates the forward mode of AD (the focus of our package), AD also has a reverse mode. Without using chain rule, it first does a forward pass to store the partial derivatives, before undertaking a reverse pass, which starts from the final function to be differentiated, $y$. After fixing the derivative of the final function, it then computes the derivative of each component function with respect to its parent function recursively (using chain rule) until the derivative of the function with respect to the basic-level argument (e.g. $x_1$) can be calculated. \n",
    "\n",
    "In terms of efficiency, the forward mode is more efficient when the number of functions to evaluate is much greater than the number of inputs, whereas the reverse mode, which computes the Jacobian-transpose-vector-product is more efficient when the number of inputs is much greater than the number of functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use `AutoDiff`\n",
    "\n",
    "At this milestone, the recommended method to access the package is to download or clone the package's Github repo (https://github.com/the-differentiators/cs207-FinalProject.git). The user must ensure that the package's requirements (`numpy`) are installed, or install them manually.\n",
    "\n",
    "By the final submission, the package will be available on PyPI and the user will be able to install the package in the standard way using `pip`. We will also provide a `yml` file which will be used by the user to create the appropriate environment and ensure that dependencies like `numpy` have been installed. Then, to use the package, the user will only need to import our package which will implicitly import any other packages used by `AutoDiff`.\n",
    "\n",
    "The following steps will walk the user through a demo of how to install and use the `AutoDiff` package:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing `AutoDiff` and requirements\n",
    "The following code is first used to change the path directory of this document to the same folder as that of the package   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to change directory to src folder\n",
    "import os\n",
    "path = os.getcwd().replace('docs','src')\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the package's Github repo has been downloaded, the `AutoDiff` package can be imported with the following code by a Python file, with its working directory configured to the same directory as `AutoDiff.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import AutoDiff as AD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this demo, we will import `numpy`, which is a requirement for the *AutoDiff* package, as well as the `Ad_Var` class from the `AutoDiff` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from AutoDiff import Ad_Var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `AutoDiff` to compute derivative of a scalar function of one variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we have included a basic demo for a scalar function, given a single input. The function used in the demo is  $f = sin^2(2x)$, which was used for illustration in the *Background* section earlier. Our objective is to use the the `Ad_Var` class to compute the value of the derivative for this function automatically, unlike the manual computational trace drawn out earlier. \n",
    "\n",
    "First, we create an instance of the `Ad_Var` object, with the value of $x = \\dfrac{\\pi}{6}$ assigned to the input variable, `val`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.pi / 6\n",
    "\n",
    "x = Ad_Var(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user should note that the `AutoDiff` package assumes that for a single input, the object being initialised will have a derivative value of 1 (stored as a Class attribute `self._ders`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create `f`, which represents the full function. The `Ad_Var` object from the previous code can be used with dunder functions and additional functions within `Ad_Var` class to construct the full function being evaluated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = (Ad_Var.sin(2*x))**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the functions are applied to the original `Ad_Var` object `x`, the `_val` and `_ders` attributes of the object are being updated with new values. The object `f`, representing the full function, will have its `_val` and `_ders` attributes containing the actual function and derivative values respectively.\n",
    "\n",
    "To note: the user also has the ability to manually set function and derivative values outside of instance initialization using the setter methods provided (`set_val` and `set_ders`). In this way, the user has the option to reuse the same objects after resetting the value and derivative(s).\n",
    "\n",
    "The associated function value and derivative(s) of any `Ad_Var` instance may be retrieved through the `get_val` and `get_ders` functions as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7499999999999999 1.7320508075688776\n"
     ]
    }
   ],
   "source": [
    "print(f.get_val(), f.get_ders())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the function value and derivative can be printed by directly printing the `Ad_Var` object associated with the function `f`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value = 0.7499999999999999\n",
      "Derivative = 1.7320508075688776\n"
     ]
    }
   ],
   "source": [
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `AutoDiff` to compute the gradient of a scalar multivariate function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the user wants to calculate the value and the gradient vector of a scalar multivariate function, then each variable must be first instantiated as an `Ad_Var` object, with inputs `val`, the scalar value of that variable, and `ders`, a `numpy` array representing the seed vector which indicates a direction along which the directional derivative of a function will be calculated. An example is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Ad_Var(1, np.array([1, 0, 0]))\n",
    "y = Ad_Var(2, np.array([0, 1, 0]))\n",
    "z = Ad_Var(3, np.array([0, 0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the user can define the function which consists of the instantiated `Ad_Var` variables. For example, below , we are calculating the value and the gradient of the function $f = sin^2(2x) + z^y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value = 9.826821810431806\n",
      "Gradient = [-1.51360499  9.8875106   6.        ]\n"
     ]
    }
   ],
   "source": [
    "f = (Ad_Var.sin(2*x))**2 + z**y\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the gradient of the function `f` is a 3-dimensional vector, since `f` is a function of 3 variables. The first dimension of the gradient vector is the directional derivative of `f` along the seed vector $[1, 0, 0]$. Since, `x` was instantiated with this seed vector, the first dimension of the gradient vector corresponds to the partial derivative $\\frac{\\partial f}{\\partial x}$ evaluated at $x=1, y=2, z=3$. Similarly, $y$ was instantiated with the seed vector $[0, 1, 0]$. In this way, the user has indicated that the second element of the gradient of `f` corresponds to $\\frac{\\partial f}{\\partial y}$ evaluated at at $x=1, y=2, z=3$. Similarly, $z$ was instantiated with the seed vector $[0, 0, 1]$, hence the third dimension of the gradient vector corresponds to $\\frac{\\partial f}{\\partial z}$ evaluated at $x=1, y=2, z=3$.\n",
    "\n",
    "In summary, each variable should be instantiated with a seed vector with dimensions equal to the dimensions of the gradient vector of the target function. For each variable, the values of the seed vector should be 0 except for one value which should be the derivative of that variable such as 1. The index of the element in the seed vector which has nonzero value indicates the index of the gradient vector which stores the partial derivative value of the target function with respect to this specific variable.  For example, if a variable is initiated with a seed vector of $[1,0,0]$, then this should be interpreted as the first variable among the three variables, and its derivative is set to be 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `AutoDiff` to compute derivative of a vector-valued multivariate function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user can also use `AutoDiff` to calculate the value and the jacobian matrix of a vector-valued function. Again the variables must be instantiated in the same way as discussed above. Then, a vector-valued function can be defined as a numpy array of functions composed of instantiated `Ad_Var` variables. An example is shown below for the vector valued function $f = \\begin{bmatrix}\n",
    "sin^2(2x) + z^y \\\\\n",
    "e^x + z\n",
    "\\end{bmatrix}$ for $x = 1, y = 2, z = 3$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Ad_Var(1, np.array([1, 0, 0]))\n",
    "y = Ad_Var(2, np.array([0, 1, 0]))\n",
    "z = Ad_Var(3, np.array([0, 0, 1]))\n",
    "\n",
    "f = np.array([(Ad_Var.sin(2*x))**2 + z**y, Ad_Var.exp(x) + z])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the user can call `get_jacobian` to get the jacobian matrix of `f` evaluated at $x = 1, y = 2, z = 3$. The first argument of this method is the vector-valued function $f$ defined as a numpy array. The second argument is the dimension of the vector of the functions (in this example the vector-valued function has 2 dimensions). The third argument is the number of variables composing the vector-valued function (in this example vector-valued function is composed of 3 variables, $x,y$ and $z$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.51360499,  9.8875106 ,  6.        ],\n",
       "       [ 2.71828183,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ad_Var.get_jacobian(f, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the user can call `get_values` by passing `f`, to calculate the value of the vector-valued function for the given values of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.82682181, 5.71828183])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ad_Var.get_values(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, the vector valued function can also be defined as a numpy array of other already instantiated functions, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = (Ad_Var.sin(2*x))**2 + z**y\n",
    "h = Ad_Var.exp(x) + z\n",
    "f = np.array([g, h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.51360499,  9.8875106 ,  6.        ],\n",
       "       [ 2.71828183,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ad_Var.get_jacobian(f, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.82682181, 5.71828183])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ad_Var.get_values(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `AutoDiff` to compute the derivatives of any type of function on a grid of points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above examples, the derivative/gradient/jacobian of a function is evaluated at a single point which is defined by the value with which each variable is instantiated. `AutoDiff`, however, can be used to evaluate the derivative/gradient/jacobian of a function on a grid of points defined by the user. The first step to do this is again to instantiate the variables with any value (please note that the default value of an `Ad_Var` variable is 1 so the value argument can be skipped)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Ad_Var(ders = np.array([1, 0, 0]))\n",
    "y = Ad_Var(ders = np.array([0, 1, 0]))\n",
    "z = Ad_Var(ders = np.array([0, 0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the user needs to define the function as a string using the same standard syntax used in any of the examples above. For example, if function is $f = sin^2(2x) + z^y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_string = \"(Ad_Var.sin(2*x))**2 + z**y\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the user can call `grid_eval` to calculate the gradient and the value of the given function on a grid of points. The first argument passed is the function string. The second argument is a list of strings where each string represents one of the variables used in the function string. The third argument is the list of the already instantiated `Ad_Var` objects which are referenced in the function string. The last argument is a list of lists defining the grid of all possible points that the user wants to calculate the gradient and the value of the function for. For example, below the function and its gradient are evaluated for all possible combinations of $(x, y, z)$ where $x \\in \\{1, 2\\}, y \\in \\{2, 3\\}, z=4$. The function returns a dictionary where each key is one of the points of the grid and the value is a tuple. The first element of the tuple is the value of the function at this point and the second element of the tuple is the gradient of the function evaluated at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 2, 4): (16.826821810431806,\n",
       "  array([-1.51360499, 22.18070978,  8.        ])),\n",
       " (1, 3, 4): (64.82682181043181,\n",
       "  array([-1.51360499, 88.72283911, 48.        ])),\n",
       " (2, 2, 4): (16.57275001690431,\n",
       "  array([ 1.97871649, 22.18070978,  8.        ])),\n",
       " (2, 3, 4): (64.57275001690431,\n",
       "  array([ 1.97871649, 88.72283911, 48.        ]))}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ad_Var.grid_eval(f_string, ['x', 'y', 'z'], [x, y, z], [[1, 2], [2,3], [4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `grid_eval` can also be used to evaluate the jacobian for vector valued functions at different points. In this case, the string representation of the vector-valued function must be written as a list of functions referencing the already instantiated `Ad_Var` variables. Please note that in this case the string representation corresponds to a list of functions and not a numpy array of functions. For example, if the user wants to evaluate the jacobian of the vector-valued function $f = \\begin{bmatrix}\n",
    "sin^2(2x) + z^y \\\\\n",
    "e^x + z\n",
    "\\end{bmatrix}$ at different points, the function string should be defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_string = \"[(Ad_Var.sin(2*x))**2 + z**y, Ad_Var.exp(x) + z]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, by calling `grid_eval` on this function string, a dictionary is returned where each key is one of the points of the grid and the value is a tuple. The first element of the tuple is the value of the function at this point and the second element of the tuple is the jacobian of the vector-valued function evaluated at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 2, 4): (array([16.82682181,  6.71828183]),\n",
       "  array([[-1.51360499, 22.18070978,  8.        ],\n",
       "         [ 2.71828183,  0.        ,  1.        ]])),\n",
       " (1, 3, 4): (array([64.82682181,  6.71828183]),\n",
       "  array([[-1.51360499, 88.72283911, 48.        ],\n",
       "         [ 2.71828183,  0.        ,  1.        ]])),\n",
       " (2, 2, 4): (array([16.57275002, 11.3890561 ]),\n",
       "  array([[ 1.97871649, 22.18070978,  8.        ],\n",
       "         [ 7.3890561 ,  0.        ,  1.        ]])),\n",
       " (2, 3, 4): (array([64.57275002, 11.3890561 ]),\n",
       "  array([[ 1.97871649, 88.72283911, 48.        ],\n",
       "         [ 7.3890561 ,  0.        ,  1.        ]]))}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ad_Var.grid_eval(f_string, ['x', 'y', 'z'], [x, y, z], [[1, 2], [2,3], [4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory structure\n",
    "Our intended directory structure is as follows:\n",
    "```\n",
    "cs207-FinalProject/\n",
    "                   README.md\n",
    "                   requirements.txt \n",
    "                   docs/\n",
    "                        milestone1.pdf\n",
    "                        milestone2.ipynb\n",
    "                   src/                   \n",
    "                        AutoDiff.py\n",
    "                   test/\n",
    "                        test_autodiff.py\n",
    "                       \n",
    "```                 \n",
    "\n",
    "### Modules\n",
    "\n",
    "The primary module will be a single `AutoDiff.py` file. Contained within will be the definition for an `Ad_Var` class. Instances of this class, through interaction with other `Ad_Var` objects, will be able to compute the value of a function as well as the value of that function's derivative with respect to any input variable. At present, we envision that this module will be powerful enough to handle forward differentiation of any function comprised of the following elementary functions:\n",
    "\n",
    "* Fundamental arithmetic operators (addition, subtraction, multiplication, and division)\n",
    "* Logarithm (of any base)\n",
    "* Negation\n",
    "* Exponentiation ($e^x$ for an `Ad_Var` instance $x$)\n",
    "* Power and root functions ($x^n$ for some real $n$)\n",
    "* Trigonometric functions ($\\sin(x)$, $\\cos(x)$, $\\tan(x)$)\n",
    "* Inverse trigonometric functions ($\\arcsin(x)$, $\\arccos(x)$, $\\arctan(x)$)\n",
    "\n",
    "Depending on our eventual choice for the \"additional\" feature of this project, or future design decisions, there\n",
    "may be additional modules added in the future that supplement or subdivide the functionality of `AutoDiff.py`.\n",
    "\n",
    "Each instance of the `Ad_Var` class in the `AutoDiff` package represents the definition of a set of variables at a particular evaluation point. Through manipulations of these instances (either through fundamental arithmetic operations or built-in methods representing additional elementary functions described earlier), a user has the capability of representing any continuous differentiable function, be it scalar or vector. This was shown earlier via a code demo.\n",
    "\n",
    "\n",
    "### Testing and Coverage\n",
    "In the `test` folder, there will be a separate Python module `test_autodiff.py`, which will be the test-suite for `AutoDiff.py`. \n",
    "\n",
    "The test-suite will contain tests for the methods in the `Ad_Var` class, to ensure that the elementary functions return the desired output. Tests are run using pytest. The tests are linked to Travis CI and CodeCov, which will manage continuous integration and code coverage respectively.\n",
    "\n",
    "### Installation and distribution of package\n",
    "At this milestone, the recommended method to access the package is to download or clone the package's Github repo (https://github.com/the-differentiators/cs207-FinalProject.git). The user must ensure that the package's requirements (`numpy`) are installed, or install them manually.\n",
    "\n",
    "By the final submission, we intend to distribute the package via PyPI. There will be no additional packaging framework included; we believe the scope of this project can be contained within a relatively simple directory structure with few functional python files and should not require additional overhead for users to install and use.\n",
    "\n",
    "The package will be available on PyPI and the user will be able to install the package in the standard way using `pip`. We will also provide a `yml` file which will be used by the user to create the appropriate environment and ensure that dependencies like `numpy` have been installed. Then, to use the package, the user will only need to import our package which will implicitly import any other packages used by `AutoDiff`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Details\n",
    "\n",
    "### Core Data Structures\n",
    "* `numpy` arrays: 1-D `numpy` arrays will be used to keep the gradient vectors as the entire trace is evaluated. `numpy`\n",
    "provides vectorized operations which will make the overloading of elementary functions much more efficient for\n",
    "multivariate functions. If a vector function is provided, 2-D `numpy` arrays will be used to hold the Jacobian matrix.\n",
    "\n",
    "* dictionaries: dictionaries were used to keep the results of `grid_eval` function call. Particularly, the keys of the dictionary are points on the grid defined by the user and the corresponding values are the function value and its derivative/gradient/jacobian at the corresponding point.\n",
    "\n",
    "### Class Implementation\n",
    "* The `Ad_Var`  class will represent the variables that are used in the Automatic Differentiation process. In the case of a single input, the instance should be initialized with, `val`, a scalar value of that variable to be evaluated on when calculating both the function and derivative values (as shown in the demo above)\n",
    "\n",
    "* In the case of multiple inputs, each input will be initialized as an `Ad_Var` object, with inputs `val`, a scalar value of that variable and `ders`, a `numpy` array representing the derivative of the input with regards to the other variables. An example is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = Ad_Var(1, np.array([1, 0, 0]))\n",
    "x2 = Ad_Var(2, np.array([0, 1, 0]))\n",
    "x3 = Ad_Var(3, np.array([0, 0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dunder methods such as \"add\" and \"mul\", and other elementary functions will be implemented under this class. More information on this is covered below in the *Class Methods* section. \n",
    "\n",
    "* As part of the class methods, we have included two static methods, `get_jacobian` and `get_values`, which respectively compute the Jacobian matrix and an array of function values for an array of `Ad_Var` objects. Also, a static method `grid_eval` is included which evaluates the function and its derivative/gradient/jacobian on a grid of points.\n",
    "\n",
    "* In our implementation, we will also use the try-except method to catch unexpected input types: for example, if the user initializes the variable value of the `Ad_Var` instance with a value of type string, which is not a valid input type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Attributes\n",
    "* `val`: float value, indicating the function value of the `Ad_Var` object evaluated at the given point\n",
    "* `ders` (for single input): float value, indicating the derivative value of `Ad_Var` object evaluated at the given point\n",
    "* `ders` (for multiple inputs): 1-D array of floats, representing the value of the derivatives of the multiple inputs evaluated at the given point \n",
    "\n",
    "* `val` and `ders` attributes will be made pseudoprivate to prevent users from manually setting function and derivative values outside of instance initialization\n",
    "\n",
    "### External Dependencies\n",
    "* `numpy` for implementation of the elementary functions (e.g. sin, sqrt, log and exp), by overloading `numpy` implementations for these functions\n",
    "* `pytest` for testing\n",
    "* TravisCI and CodeCov used to manage continuous integration and code coverage\n",
    "\n",
    "### Class Methods\n",
    "\n",
    "1. `__init__(self, val, ders=1)`:\n",
    "    * Sets `self._val` to the argument `val`\n",
    "    * Sets `self._ders` to the argument `ders`\n",
    "    \n",
    "    \n",
    "2. `__eq__(self, other)`:\n",
    "    * Returns True if `self._val` == `other._val` and `self._ders` == `other._ders`, returns False otherwise\n",
    "    \n",
    "    \n",
    "3. `__ne__(self, other)`:\n",
    "    * Returns True if `self._val` != `other._val` or `self._ders` != `other._ders`, returns False otherwise\n",
    "    \n",
    "    \n",
    "4. `__neg__(self)`:\n",
    "    * Returns an `Ad_Var` object with `self._val` and `self._ders` negated\n",
    "\n",
    "\n",
    "5. `_repr__(self)`:\n",
    "    * Returns a string representing the value of `self._val` (Value) and the value of `self._ders` (Gradient)\n",
    "    \n",
    "    \n",
    "6. `set_val(self, value)`:\n",
    "    * Set the value of the private attribute `self._val` with `value`\n",
    "\n",
    "6. `set_ders(self, derivatives)`:\n",
    "    * Set the value of the privae attribute `self._ders` with `derivatives`\n",
    "    \n",
    "    \n",
    "8. `get_ders(self, derivatives)`:\n",
    "    * Set the value of the attribute `self._ders` with `derivatives`\n",
    "\n",
    "\n",
    "9. `get_val(self)`:\n",
    "    * Returns the value of the attribute `self._val` \n",
    "    \n",
    "    \n",
    "10. `get_ders(self)`:\n",
    "    * Returns the value of the attribute `self._ders` \n",
    "\n",
    "\n",
    "11. `__add__(self, other)` and `__radd__(self, other)`:\n",
    "    * Other can be a float, int or AutoDiff object\n",
    "    * Returns an `Ad_Var` object when calculating self + other or other + self\n",
    "\n",
    "\n",
    "12. `__sub__(self, other)` and `__rsub__(self, other)`:\n",
    "    * Other can be a float, int or AutoDiff object\n",
    "    * Returns an `Ad_Var` object when calculating self - other or other - self\n",
    "\n",
    "\n",
    "13. `__mul__(self, other)` and `__rmul__(self, other)`:\n",
    "    * Other can be a float, int or AutoDiff object\n",
    "    * Returns an `Ad_Var` object when calculating self * other or other * self\n",
    "    \n",
    "\n",
    "14. `__truediv__(self, other)` and `__rtruediv__(self, other)`:\n",
    "    * Other can be a float, int or AutoDiff object\n",
    "    * Returns an `Ad_Var` object when calculating self / other or other / self\n",
    "\n",
    "\n",
    "15. `__pow__(self, other)` and `__rpow__(self, other)`:\n",
    "    * `other` can be a float, int or `Ad_Var` object\n",
    "    * `__rpow__` will require `other` to be a numeric type, otherwise, it will raise a TypeError\n",
    "    * Returns an `Ad_Var` object when calculating self ** other\n",
    "\n",
    "\n",
    "16. `sqrt(self)`:\n",
    "    * Returns an `Ad_Var` object by calling the __pow__ method using self**0.5\n",
    "    \n",
    "\n",
    "17. `exp(self)`:\n",
    "    * Returns an `Ad_Var` object with `self._val = np.exp(self._val)` and `self._deres = np.exp(self._val) * self._ders`\n",
    "\n",
    "\n",
    "18. `log(self, logbase=np.e)`:\n",
    "    * Optional argument for `logbase` (can be a float or int). By default, `logbase` is set to the exponential.\n",
    "    * Returns an `Ad_Var` object with `self._val = np.log(self._val) / np.log(logbase)` and `self._ders = self._ders / (self._val * np.log(logbase))`\n",
    "\n",
    "\n",
    "19. `sin(self)` and `cos(self)` and `tan(self)`:\n",
    "    * Returns an `Ad_Var` object with `self._val` and `self._ders` updated accordingly based on the given trigonometric function\n",
    "    \n",
    "    \n",
    "20. `arcsin(self)` and `arccos(self)` and `arctan(self)`:\n",
    "    * Returns an `Ad_Var` object with `self._val` and `self._ders` updated accordingly based on the given inverse trigonometric function  \n",
    "\n",
    "\n",
    "21. `sinh(self)` and `cosh(self)` and `tanh(self)`:\n",
    "    * Returns an `Ad_Var` object with `self._val` and `self._ders` updated accordingly based on the given hyperbolic function\n",
    "\n",
    "\n",
    "22. `logistic(self)`: \n",
    "    * Returns an `Ad_Var` object with `self._val` and `self._ders` updated accordingly based on the logistic (sigmoid) function\n",
    "    \n",
    "    \n",
    "23. `get_jacobian(functions_array, functions_dim, vars_dim)`:\n",
    "    * Static method that returns the Jacobian matrix for a given array of `Ad_Var` objects \n",
    "    \n",
    "    \n",
    "24. `get_values(functions_array)`:\n",
    "    * Static method that returns the an array of function values for a given array of `Ad_Var` objects\n",
    "   \n",
    "   \n",
    "25. `grid_eval(func_string, vars_strings, Ad_Vars_list, grid)`:\n",
    "    * Static method that evaluates a function and its derivative/gradient/jacobian on a grid of points. A dictionary is returned where each key is a point of the grid and the value is a tuple with the first element being the value of the function at this point and second element is the derivative/gradient/jacobian evaluated at this point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Features\n",
    "\n",
    "Our next steps will be to implement the reverse mode of Automatic Differentiation. The main challenge for this is to translate the conceptual framework of reverse mode into functioning code that builds on existing code. This might require creating a new class specifically for the reverse mode that replicates similar functions already in the existing `Ad_Var`. \n",
    "\n",
    "Building on our implementation of reverse mode of Auto Differentiation, a future feature that we plan to implement would be a feature that automatically chooses between the forward mode or reverse mode of Automatic Differentiation, based on what is optimal based on the number of parameters and functions. This will be presented to the user of the package to facilitate their decision-making.\n",
    "\n",
    "In terms of the forward mode of Auto Differentiation, we will work on refining the `get_jacobian` method that would return the Jacobian matrix for an array of `Ad_Var` objects. This will allow the user to access the Jacobian via the `Ad_Var` class. We will also include doc-strings for the code, to ensure that the code is accessible to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "* [A Hitchhiker’s Guide to Automatic Differentiation](https://link.springer.com/article/10.1007/s11075-015-0067-6)\n",
    "* Harvard CS207 2019 course materials"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
